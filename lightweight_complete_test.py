import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import time
import pandas as pd
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æ¨¡å‹å’Œè¯„ä¼°æ¡†æ¶
from model_a_gnn import ModelA_GNN, ModelALoss
from model_b_similarity import ModelB_Similarity, ModelBLoss
from data_generation import SyntheticDataGenerator
from evaluation_framework import GraphPostProcessor, SpectralClusteringEvaluator, ModularityCalculator


class LightweightCompleteTest:
    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        
        # ä¿æŒå®Œæ•´çš„æ¨¡å‹é…ç½®ï¼Œä½†ä½¿ç”¨é€‚ä¸­çš„å‚æ•°
        self.model_config_a = {
            'input_channels': 1,
            'feature_dim': 128,    # é€‚ä¸­çš„ç‰¹å¾ç»´åº¦
            'max_nodes': 150,      # é€‚ä¸­çš„æœ€å¤§èŠ‚ç‚¹æ•°
            'coord_dim': 2,
            'hidden_dim': 32,      # é€‚ä¸­çš„éšè—å±‚ç»´åº¦
            'node_feature_dim': 32 # é€‚ä¸­çš„èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
        }
        
        self.model_config_b = {
            'input_channels': 1,
            'feature_dim': 128,    # é€‚ä¸­çš„ç‰¹å¾ç»´åº¦
            'max_nodes': 150,      # é€‚ä¸­çš„æœ€å¤§èŠ‚ç‚¹æ•°
            'coord_dim': 2,
            'similarity_hidden_dim': 16  # é€‚ä¸­çš„éšè—å±‚ç»´åº¦
        }
        
        # è½»é‡çº§è®­ç»ƒé…ç½®
        self.training_config = {
            'batch_size': 8,       # å°æ‰¹æ¬¡
            'learning_rate': 0.001, # æ ‡å‡†å­¦ä¹ ç‡
            'epochs': 10,          # è¾ƒå°‘çš„è®­ç»ƒè½®æ•°
            'weight_decay': 1e-5
        }
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model_a = ModelA_GNN(**self.model_config_a).to(self.device)
        self.model_b = ModelB_Similarity(**self.model_config_b).to(self.device)
        
        # æŸå¤±å‡½æ•°
        self.loss_a = ModelALoss(coord_weight=1.0, edge_weight=1.0, count_weight=0.1)
        self.loss_b = ModelBLoss(coord_weight=1.0, edge_weight=1.0, count_weight=0.1, similarity_weight=0.5)
        
        # ä¼˜åŒ–å™¨
        self.optimizer_a = optim.Adam(self.model_a.parameters(), 
                                    lr=self.training_config['learning_rate'], 
                                    weight_decay=self.training_config['weight_decay'])
        self.optimizer_b = optim.Adam(self.model_b.parameters(), 
                                    lr=self.training_config['learning_rate'], 
                                    weight_decay=self.training_config['weight_decay'])
        
        # è¯„ä¼°ç»„ä»¶
        self.graph_processor = GraphPostProcessor(k_top_edges=8)
        self.spectral_evaluator = SpectralClusteringEvaluator(n_clusters=2, random_state=42)
        self.modularity_calculator = ModularityCalculator()
        
        # ç»“æœå­˜å‚¨
        self.results = {
            'model_a': {'metrics': [], 'inference_times': [], 'train_losses': []},
            'model_b': {'metrics': [], 'inference_times': [], 'train_losses': []}
        }
    
    def generate_dataset(self, n_samples: int, dataset_type: str) -> List[Dict]:
        """ç”Ÿæˆæ•°æ®é›†"""
        dataset = []
        
        for i in range(n_samples):
            n_points = np.random.randint(100, 151)  # é€‚ä¸­çš„èŠ‚ç‚¹æ•°é‡
            noise_level = np.random.uniform(0.05, 0.15)
            
            temp_generator = SyntheticDataGenerator(
                img_size=64, 
                n_samples=n_points, 
                noise=noise_level
            )
            data = temp_generator.generate_dataset(dataset_type, n_points)
            dataset.append(data)
        
        return dataset
    
    def prepare_batch(self, batch_data: List[Dict]) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """å‡†å¤‡æ‰¹æ¬¡æ•°æ®"""
        batch_size = len(batch_data)
        max_nodes = self.model_config_a['max_nodes']
        
        # å‡†å¤‡å›¾åƒæ•°æ®
        images = torch.stack([torch.from_numpy(data['image']).unsqueeze(0) for data in batch_data])
        
        # å‡†å¤‡ç›®æ ‡æ•°æ®
        targets = {
            'points': torch.zeros(batch_size, max_nodes, 2),
            'node_masks': torch.zeros(batch_size, max_nodes, dtype=torch.bool),
            'adjacency': torch.zeros(batch_size, max_nodes, max_nodes),
            'labels': []
        }
        
        for i, data in enumerate(batch_data):
            n_points = len(data['points'])
            n_valid = min(n_points, max_nodes)
            
            # å¡«å……åæ ‡
            targets['points'][i, :n_valid] = torch.from_numpy(data['points'][:n_valid])
            
            # å¡«å……èŠ‚ç‚¹æ©ç 
            targets['node_masks'][i, :n_valid] = True
            
            # å¡«å……é‚»æ¥çŸ©é˜µ
            adj = torch.from_numpy(data['adjacency'][:n_valid, :n_valid])
            targets['adjacency'][i, :n_valid, :n_valid] = adj
            
            # å­˜å‚¨çœŸå®æ ‡ç­¾
            targets['labels'].append(data['labels'][:n_valid])
        
        return images, targets
    
    def train_model(self, model: nn.Module, optimizer: optim.Optimizer, loss_fn: nn.Module, 
                   train_loader: DataLoader, model_name: str) -> List[float]:
        """è®­ç»ƒæ¨¡å‹"""
        model.train()
        train_losses = []
        
        total_batches = len(train_loader)
        print(f"å¼€å§‹è®­ç»ƒ{model_name}ï¼Œå…±{self.training_config['epochs']}ä¸ªepochï¼Œæ¯epoch {total_batches}ä¸ªbatch")
        
        for epoch in range(self.training_config['epochs']):
            epoch_loss = 0.0
            num_batches = 0
            
            print(f"\n{model_name} Epoch {epoch+1}/{self.training_config['epochs']}:")
            
            for batch_idx, batch_data in enumerate(train_loader):
                images, targets = self.prepare_batch(batch_data)
                images = images.to(self.device)
                targets = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in targets.items()}
                
                optimizer.zero_grad()
                
                predictions = model(images, targets['node_masks'])
                loss_dict = loss_fn(predictions, targets)
                total_loss = sum(loss_dict.values())
                
                total_loss.backward()
                optimizer.step()
                
                epoch_loss += total_loss.item()
                num_batches += 1
                
                # æ˜¾ç¤ºæ¯ä¸ªbatchçš„è¿›åº¦
                if (batch_idx + 1) % 2 == 0 or batch_idx == total_batches - 1:
                    current_loss = total_loss.item()
                    progress = (batch_idx + 1) / total_batches * 100
                    print(f"  Batch {batch_idx+1}/{total_batches} ({progress:.1f}%) - Loss: {current_loss:.4f}")
            
            avg_loss = epoch_loss / num_batches
            train_losses.append(avg_loss)
            
            print(f"  Epoch {epoch+1} å®Œæˆ - å¹³å‡Loss: {avg_loss:.4f}")
            
            # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ¡
            progress_bar = "â–ˆ" * int((epoch + 1) / self.training_config['epochs'] * 20) + "â–‘" * (20 - int((epoch + 1) / self.training_config['epochs'] * 20))
            print(f"  è®­ç»ƒè¿›åº¦: [{progress_bar}] {epoch+1}/{self.training_config['epochs']}")
        
        print(f"\n{model_name} è®­ç»ƒå®Œæˆï¼")
        return train_losses
    
    def evaluate_model_on_moons(self, model: nn.Module, test_data: List[Dict], model_name: str):
        """åœ¨åŒæœˆæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
        print(f"\n=== è¯„ä¼°{model_name}åœ¨åŒæœˆæ•°æ®é›†ä¸Šçš„è¡¨ç° ===")
        print(f"å…±{len(test_data)}ä¸ªæµ‹è¯•æ ·æœ¬")
        
        all_metrics = []
        all_inference_times = []
        
        for i, data in enumerate(test_data):
            # æ˜¾ç¤ºè¿›åº¦
            progress = (i + 1) / len(test_data) * 100
            progress_bar = "â–ˆ" * int(progress / 5) + "â–‘" * (20 - int(progress / 5))
            print(f"è¯„ä¼°è¿›åº¦: [{progress_bar}] {i+1}/{len(test_data)} ({progress:.1f}%)")
            
            # å‡†å¤‡æ•°æ®
            image = torch.from_numpy(data['image']).unsqueeze(0).unsqueeze(0).to(self.device)  # [1, 1, 64, 64]
            n_points = len(data['points'])
            n_valid = min(n_points, self.model_config_a['max_nodes'])
            
            node_mask = torch.zeros(self.model_config_a['max_nodes'], dtype=torch.bool)
            node_mask[:n_valid] = True
            node_mask = node_mask.to(self.device)
            
            true_labels = data['labels'][:n_valid]
            
            # è¯„ä¼°
            model.eval()
            with torch.no_grad():
                start_time = time.time()
                predictions = model(image, node_mask.unsqueeze(0))
                end_time = time.time()
                inference_time = (end_time - start_time) * 1000
                
                # è·å–é‚»æ¥çŸ©é˜µ
                adjacency = predictions['adjacency_matrix'][0].cpu().numpy()
                
                # å¤„ç†é‚»æ¥çŸ©é˜µ
                processed_adjacency = self.graph_processor.process(adjacency)
                
                # è°±èšç±»
                predicted_labels = self.spectral_evaluator.cluster(processed_adjacency)
                
                # ç¡®ä¿æ ‡ç­¾é•¿åº¦åŒ¹é…
                n_valid = min(len(predicted_labels), len(true_labels))
                predicted_labels = predicted_labels[:n_valid]
                true_labels = true_labels[:n_valid]
                
                # è®¡ç®—æŒ‡æ ‡
                clustering_metrics = self.spectral_evaluator.evaluate_clustering(predicted_labels, true_labels)
                modularity = self.modularity_calculator.calculate_modularity(processed_adjacency, predicted_labels)
                
                metrics = {
                    'ari': clustering_metrics['ari'],
                    'nmi': clustering_metrics['nmi'],
                    'modularity': modularity,
                    'inference_time_ms': inference_time
                }
                
                all_metrics.append(metrics)
                all_inference_times.append(inference_time)
                
                print(f"  æ ·æœ¬{i+1} - ARI: {metrics['ari']:.3f}, NMI: {metrics['nmi']:.3f}, æ—¶é—´: {inference_time:.1f}ms")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {
            'ari': np.mean([m['ari'] for m in all_metrics]),
            'nmi': np.mean([m['nmi'] for m in all_metrics]),
            'modularity': np.mean([m['modularity'] for m in all_metrics]),
            'inference_time_ms': np.mean(all_inference_times)
        }
        
        self.results[model_name]['metrics'] = all_metrics
        self.results[model_name]['inference_times'] = all_inference_times
        
        print(f"âœ… {model_name} è¯„ä¼°å®Œæˆ")
        print(f"  å¹³å‡ARI: {avg_metrics['ari']:.3f}")
        print(f"  å¹³å‡NMI: {avg_metrics['nmi']:.3f}")
        print(f"  å¹³å‡Modularity: {avg_metrics['modularity']:.3f}")
        print(f"  å¹³å‡æ¨ç†æ—¶é—´: {avg_metrics['inference_time_ms']:.1f}ms")
        
        return avg_metrics
    
    def run_lightweight_complete_test(self):
        """è¿è¡Œè½»é‡çº§å®Œæ•´æµ‹è¯•"""
        print("="*60)
        print("è½»é‡çº§å®Œæ•´åŒæœˆæ•°æ®é›†æ³›åŒ–èƒ½åŠ›æµ‹è¯•")
        print("="*60)
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹Aå‚æ•°æ•°é‡: {self.model_a.count_parameters():,}")
        print(f"æ¨¡å‹Bå‚æ•°æ•°é‡: {self.model_b.count_parameters():,}")
        
        # ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆåœ†å½¢æ•°æ®ï¼‰
        print("\nğŸ“Š ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆåœ†å½¢æ•°æ®é›†ï¼‰...")
        train_data = self.generate_dataset(20, 'circles')  # 20ä¸ªè®­ç»ƒæ ·æœ¬
        print("âœ… è®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆ")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼ˆåŒæœˆæ•°æ®ï¼‰
        print("\nğŸ“Š ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼ˆåŒæœˆæ•°æ®é›†ï¼‰...")
        test_data = self.generate_dataset(12, 'moons')  # 12ä¸ªæµ‹è¯•æ ·æœ¬
        print("âœ… æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_data, batch_size=self.training_config['batch_size'], 
                                shuffle=True, collate_fn=lambda x: x)
        
        print(f"\nğŸš€ å¼€å§‹è½»é‡çº§è®­ç»ƒ...")
        print(f"è®­ç»ƒé…ç½®: {self.training_config['epochs']} epochs, batch_size={self.training_config['batch_size']}")
        
        # è®­ç»ƒæ¨¡å‹A
        print("\n" + "="*40)
        print("ğŸ”§ è®­ç»ƒæ¨¡å‹A (GNN-based)")
        print("="*40)
        start_time = time.time()
        train_losses_a = self.train_model(self.model_a, self.optimizer_a, self.loss_a, train_loader, "Model A")
        train_time_a = time.time() - start_time
        print(f"âœ… æ¨¡å‹Aè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {train_time_a:.2f}ç§’")
        self.results['model_a']['train_losses'] = train_losses_a
        
        # è®­ç»ƒæ¨¡å‹B
        print("\n" + "="*40)
        print("ğŸ”§ è®­ç»ƒæ¨¡å‹B (Similarity-based)")
        print("="*40)
        start_time = time.time()
        train_losses_b = self.train_model(self.model_b, self.optimizer_b, self.loss_b, train_loader, "Model B")
        train_time_b = time.time() - start_time
        print(f"âœ… æ¨¡å‹Bè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {train_time_b:.2f}ç§’")
        self.results['model_b']['train_losses'] = train_losses_b
        
        print(f"\nğŸ“ˆ å¼€å§‹è¯„ä¼°é˜¶æ®µ...")
        
        # è¯„ä¼°æ¨¡å‹A
        metrics_a = self.evaluate_model_on_moons(self.model_a, test_data, 'model_a')
        
        # è¯„ä¼°æ¨¡å‹B
        metrics_b = self.evaluate_model_on_moons(self.model_b, test_data, 'model_b')
        
        # åˆ†æç»“æœ
        self.analyze_results()
        
        # ä¿å­˜ç»“æœ
        self.save_results()
    
    def analyze_results(self):
        """åˆ†æç»“æœ"""
        print("\n" + "="*60)
        print("è½»é‡çº§å®Œæ•´æ³›åŒ–èƒ½åŠ›åˆ†æç»“æœ")
        print("="*60)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        for model_name in ['model_a', 'model_b']:
            metrics = self.results[model_name]['metrics']
            
            ari_values = [m['ari'] for m in metrics]
            nmi_values = [m['nmi'] for m in metrics]
            modularity_values = [m['modularity'] for m in metrics]
            inference_times = [m['inference_time_ms'] for m in metrics]
            
            print(f"\n{model_name.upper()} ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  ARI: {np.mean(ari_values):.3f} Â± {np.std(ari_values):.3f}")
            print(f"  NMI: {np.mean(nmi_values):.3f} Â± {np.std(nmi_values):.3f}")
            print(f"  Modularity: {np.mean(modularity_values):.3f} Â± {np.std(modularity_values):.3f}")
            print(f"  æ¨ç†æ—¶é—´: {np.mean(inference_times):.1f} Â± {np.std(inference_times):.1f} ms")
        
        # æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹
        print("\næ¨¡å‹æ¯”è¾ƒ:")
        model_a_ari = np.mean([m['ari'] for m in self.results['model_a']['metrics']])
        model_b_ari = np.mean([m['ari'] for m in self.results['model_b']['metrics']])
        
        if model_a_ari > model_b_ari:
            print(f"âœ… æ¨¡å‹Aåœ¨ARIä¸Šè¡¨ç°æ›´å¥½: {model_a_ari:.3f} vs {model_b_ari:.3f}")
        else:
            print(f"âœ… æ¨¡å‹Båœ¨ARIä¸Šè¡¨ç°æ›´å¥½: {model_b_ari:.3f} vs {model_a_ari:.3f}")
        
        # æ³›åŒ–èƒ½åŠ›è¯„ä¼°
        print("\næ³›åŒ–èƒ½åŠ›è¯„ä¼°:")
        if model_a_ari > 0.3:
            print(f"âœ… æ¨¡å‹Aåœ¨åŒæœˆæ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ› (ARI > 0.3)")
        elif model_a_ari > 0.2:
            print(f"âš ï¸  æ¨¡å‹Aåœ¨åŒæœˆæ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºä¸€å®šçš„æ³›åŒ–èƒ½åŠ› (0.2 < ARI â‰¤ 0.3)")
        else:
            print(f"âŒ æ¨¡å‹Aåœ¨åŒæœˆæ•°æ®é›†ä¸Šæ³›åŒ–èƒ½åŠ›æœ‰é™ (ARI â‰¤ 0.2)")
        
        if model_b_ari > 0.3:
            print(f"âœ… æ¨¡å‹Båœ¨åŒæœˆæ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ› (ARI > 0.3)")
        elif model_b_ari > 0.2:
            print(f"âš ï¸  æ¨¡å‹Båœ¨åŒæœˆæ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºä¸€å®šçš„æ³›åŒ–èƒ½åŠ› (0.2 < ARI â‰¤ 0.3)")
        else:
            print(f"âŒ æ¨¡å‹Båœ¨åŒæœˆæ•°æ®é›†ä¸Šæ³›åŒ–èƒ½åŠ›æœ‰é™ (ARI â‰¤ 0.2)")
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        # åˆ›å»ºç»“æœDataFrame
        results_data = []
        
        for model_name in ['model_a', 'model_b']:
            metrics = self.results[model_name]['metrics']
            
            avg_ari = np.mean([m['ari'] for m in metrics])
            avg_nmi = np.mean([m['nmi'] for m in metrics])
            avg_modularity = np.mean([m['modularity'] for m in metrics])
            avg_inference_time = np.mean([m['inference_time_ms'] for m in metrics])
            
            # åˆ¤æ–­æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            meets_ari_target = avg_ari > 0.3
            beats_sklearn = avg_inference_time < 100
            
            results_data.append({
                'Method': model_name.replace('_', ' ').title(),
                'ARI': avg_ari,
                'NMI': avg_nmi,
                'Modularity': avg_modularity,
                'Inference_Time_ms': avg_inference_time,
                'Meets_ARI_Target': meets_ari_target,
                'Beats_Sklearn': beats_sklearn,
                'Description': f'Lightweight complete {model_name.replace("_", " ").title()} on moons dataset'
            })
        
        # ä¿å­˜åˆ°CSV
        df = pd.DataFrame(results_data)
        df.to_csv('lightweight_complete_moons_results.csv', index=False)
        
        print(f"\nç»“æœå·²ä¿å­˜åˆ° lightweight_complete_moons_results.csv")
        print(df.to_string(index=False))


def main():
    """ä¸»å‡½æ•°"""
    evaluator = LightweightCompleteTest()
    evaluator.run_lightweight_complete_test()


if __name__ == "__main__":
    main() 