"""
å…³é”®è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ¨¡å—
åŒ…å«ARI, NMI, Modularity, Inference_Time_msç­‰å›¾åˆ†ææŒ‡æ ‡
"""

import torch
import numpy as np
import time
from typing import Dict, List, Tuple, Optional
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
from sklearn.cluster import SpectralClustering, KMeans
import networkx as nx
from scipy.sparse import csr_matrix
import warnings
warnings.filterwarnings('ignore')


class GraphEvaluationMetrics:
    """å›¾è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨"""
    
    def __init__(self, device: str = 'cuda'):
        self.device = device
        
    def extract_communities_from_adjacency(self, adjacency: torch.Tensor, 
                                         node_mask: torch.Tensor,
                                         method: str = 'spectral',
                                         n_clusters: Optional[int] = None) -> np.ndarray:
        """ä»é‚»æ¥çŸ©é˜µæå–ç¤¾åŒºæ ‡ç­¾"""
        # è·å–æœ‰æ•ˆèŠ‚ç‚¹çš„é‚»æ¥çŸ©é˜µ
        valid_nodes = node_mask.sum().item()
        if valid_nodes <= 1:
            return np.array([0])
            
        adj_matrix = adjacency[:valid_nodes, :valid_nodes].detach().cpu().numpy()
        
        # ç¡®ä¿é‚»æ¥çŸ©é˜µå¯¹ç§°
        adj_matrix = (adj_matrix + adj_matrix.T) / 2
        
        # è‡ªåŠ¨ç¡®å®šèšç±»æ•°é‡
        if n_clusters is None:
            n_clusters = min(max(2, valid_nodes // 10), 10)
        
        n_clusters = min(n_clusters, valid_nodes)
        
        try:
            if method == 'spectral':
                # è°±èšç±»
                clustering = SpectralClustering(
                    n_clusters=n_clusters,
                    affinity='precomputed',
                    random_state=42,
                    n_init=10
                )
                labels = clustering.fit_predict(adj_matrix)
            elif method == 'kmeans':
                # åŸºäºé‚»æ¥çŸ©é˜µçš„K-means
                clustering = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                labels = clustering.fit_predict(adj_matrix)
            else:
                # é»˜è®¤ä½¿ç”¨è°±èšç±»
                clustering = SpectralClustering(
                    n_clusters=n_clusters,
                    affinity='precomputed',
                    random_state=42,
                    n_init=10
                )
                labels = clustering.fit_predict(adj_matrix)
                
        except Exception as e:
            # å¦‚æœèšç±»å¤±è´¥ï¼Œè¿”å›ç®€å•çš„æ ‡ç­¾
            labels = np.arange(valid_nodes) % n_clusters
            
        return labels
    
    def calculate_modularity(self, adjacency: torch.Tensor, 
                           node_mask: torch.Tensor,
                           communities: np.ndarray) -> float:
        """è®¡ç®—æ¨¡å—åº¦ (Modularity)"""
        valid_nodes = node_mask.sum().item()
        if valid_nodes <= 1:
            return 0.0
            
        adj_matrix = adjacency[:valid_nodes, :valid_nodes].detach().cpu().numpy()
        
        try:
            # åˆ›å»ºNetworkXå›¾
            G = nx.from_numpy_array(adj_matrix)
            
            # åˆ›å»ºç¤¾åŒºå­—å…¸
            community_dict = {}
            for node, community in enumerate(communities):
                if community not in community_dict:
                    community_dict[community] = []
                community_dict[community].append(node)
            
            # è®¡ç®—æ¨¡å—åº¦
            modularity = nx.community.modularity(G, community_dict.values())
            
        except Exception as e:
            # å¦‚æœNetworkXå¤±è´¥ï¼Œä½¿ç”¨ç®€å•è®¡ç®—
            modularity = self._simple_modularity(adj_matrix, communities)
            
        return float(modularity)
    
    def _simple_modularity(self, adj_matrix: np.ndarray, communities: np.ndarray) -> float:
        """ç®€å•æ¨¡å—åº¦è®¡ç®—"""
        n = adj_matrix.shape[0]
        m = np.sum(adj_matrix) / 2  # è¾¹æ•°
        
        if m == 0:
            return 0.0
            
        modularity = 0.0
        for c in np.unique(communities):
            nodes_in_c = np.where(communities == c)[0]
            
            # ç¤¾åŒºå†…è¾¹æ•°
            l_c = np.sum(adj_matrix[np.ix_(nodes_in_c, nodes_in_c)]) / 2
            
            # ç¤¾åŒºåº¦æ•°
            d_c = np.sum(adj_matrix[nodes_in_c, :])
            
            modularity += l_c / m - (d_c / (2 * m)) ** 2
            
        return modularity
    
    def calculate_ari(self, true_labels: np.ndarray, pred_labels: np.ndarray) -> float:
        """è®¡ç®—è°ƒæ•´å…°å¾·æŒ‡æ•° (ARI)"""
        if len(true_labels) != len(pred_labels) or len(true_labels) <= 1:
            return 0.0
            
        try:
            ari = adjusted_rand_score(true_labels, pred_labels)
        except Exception:
            ari = 0.0
            
        return float(ari)
    
    def calculate_nmi(self, true_labels: np.ndarray, pred_labels: np.ndarray) -> float:
        """è®¡ç®—æ ‡å‡†åŒ–äº’ä¿¡æ¯ (NMI)"""
        if len(true_labels) != len(pred_labels) or len(true_labels) <= 1:
            return 0.0
            
        try:
            nmi = normalized_mutual_info_score(true_labels, pred_labels)
        except Exception:
            nmi = 0.0
            
        return float(nmi)
    
    def measure_inference_time(self, model, images: torch.Tensor, 
                             node_masks: torch.Tensor, 
                             warmup_runs: int = 5, 
                             measurement_runs: int = 10) -> float:
        """æµ‹é‡æ¨ç†æ—¶é—´ (æ¯«ç§’)"""
        model.eval()
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(warmup_runs):
                _ = model(images, node_masks)
        
        # æµ‹é‡
        torch.cuda.synchronize() if self.device == 'cuda' else None
        
        times = []
        with torch.no_grad():
            for _ in range(measurement_runs):
                start_time = time.perf_counter()
                _ = model(images, node_masks)
                torch.cuda.synchronize() if self.device == 'cuda' else None
                end_time = time.perf_counter()
                times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        
        return float(np.mean(times))
    
    def create_ground_truth_labels(self, adjacency: torch.Tensor, 
                                 node_mask: torch.Tensor) -> np.ndarray:
        """ä¸ºçœŸå®é‚»æ¥çŸ©é˜µåˆ›å»ºå‚è€ƒç¤¾åŒºæ ‡ç­¾"""
        # è¿™é‡Œä½¿ç”¨è°±èšç±»ä½œä¸º"çœŸå®"æ ‡ç­¾çš„å‚è€ƒ
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥ä½¿ç”¨çœŸå®çš„ç¤¾åŒºæ ‡ç­¾
        return self.extract_communities_from_adjacency(
            adjacency, node_mask, method='spectral'
        )
    
    def evaluate_batch(self, model, batch: Dict[str, torch.Tensor], 
                      clustering_method: str = 'spectral') -> Dict[str, float]:
        """è¯„ä¼°ä¸€ä¸ªæ‰¹æ¬¡çš„æ‰€æœ‰æŒ‡æ ‡"""
        images = batch['image'].to(self.device)
        points = batch['points'].to(self.device)
        adjacency_true = batch['adjacency'].to(self.device)
        node_masks = batch['node_mask'].to(self.device)
        
        batch_size = images.shape[0]
        
        # æ¨ç†æ—¶é—´æµ‹é‡
        inference_time = self.measure_inference_time(model, images, node_masks)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            predictions = model(images, node_masks)
        
        adjacency_pred = predictions['adjacency_matrix']
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŒ‡æ ‡
        ari_scores = []
        nmi_scores = []
        modularity_scores = []
        
        for b in range(batch_size):
            node_mask = node_masks[b]
            valid_nodes = node_mask.sum().item()
            
            if valid_nodes <= 1:
                continue
                
            # è·å–çœŸå®å’Œé¢„æµ‹çš„é‚»æ¥çŸ©é˜µ
            adj_true = adjacency_true[b]
            adj_pred = adjacency_pred[b]
            
            # æå–ç¤¾åŒºæ ‡ç­¾
            true_labels = self.create_ground_truth_labels(adj_true, node_mask)
            pred_labels = self.extract_communities_from_adjacency(
                adj_pred, node_mask, method=clustering_method
            )
            
            # è®¡ç®—æŒ‡æ ‡
            ari = self.calculate_ari(true_labels, pred_labels)
            nmi = self.calculate_nmi(true_labels, pred_labels)
            modularity = self.calculate_modularity(adj_pred, node_mask, pred_labels)
            
            ari_scores.append(ari)
            nmi_scores.append(nmi)
            modularity_scores.append(modularity)
        
        # è¿”å›å¹³å‡æŒ‡æ ‡
        metrics = {
            'ARI': np.mean(ari_scores) if ari_scores else 0.0,
            'NMI': np.mean(nmi_scores) if nmi_scores else 0.0,
            'Modularity': np.mean(modularity_scores) if modularity_scores else 0.0,
            'Inference_Time_ms': inference_time
        }
        
        return metrics

    def compute_batch_metrics(self, pred_adj: np.ndarray, true_adj: np.ndarray, 
                            masks: np.ndarray, clustering_method: str = 'spectral') -> Dict[str, List[float]]:
        """
        è®¡ç®—æ‰¹æ¬¡æŒ‡æ ‡ï¼Œä¸è®­ç»ƒè„šæœ¬æ¥å£å…¼å®¹
        
        Args:
            pred_adj: é¢„æµ‹çš„é‚»æ¥çŸ©é˜µ [batch_size, max_nodes, max_nodes]
            true_adj: çœŸå®çš„é‚»æ¥çŸ©é˜µ [batch_size, max_nodes, max_nodes]
            masks: èŠ‚ç‚¹æ©ç  [batch_size, max_nodes]
            
        Returns:
            Dictionary containing lists of metrics for each sample
        """
        batch_size = pred_adj.shape[0]
        
        # åˆå§‹åŒ–æŒ‡æ ‡åˆ—è¡¨
        ari_scores = []
        nmi_scores = []
        modularity_scores = []
        inference_times = []
        
        for b in range(batch_size):
            node_mask = torch.from_numpy(masks[b]).bool()
            valid_nodes = node_mask.sum().item()
            
            if valid_nodes <= 1:
                # è·³è¿‡æ— æ•ˆæ ·æœ¬
                ari_scores.append(0.0)
                nmi_scores.append(0.0)
                modularity_scores.append(0.0)
                inference_times.append(0.0)
                continue
                
            # è½¬æ¢ä¸ºå¼ é‡
            adj_true = torch.from_numpy(true_adj[b]).float()
            adj_pred = torch.from_numpy(pred_adj[b]).float()
            
            # æå–ç¤¾åŒºæ ‡ç­¾
            true_labels = self.create_ground_truth_labels(adj_true, node_mask)
            pred_labels = self.extract_communities_from_adjacency(
                adj_pred, node_mask, method=clustering_method
            )
            
            # è®¡ç®—æŒ‡æ ‡
            ari = self.calculate_ari(true_labels, pred_labels)
            nmi = self.calculate_nmi(true_labels, pred_labels)
            modularity = self.calculate_modularity(adj_pred, node_mask, pred_labels)
            
            ari_scores.append(ari)
            nmi_scores.append(nmi)
            modularity_scores.append(modularity)
            inference_times.append(1.0)  # å ä½ç¬¦ï¼Œå®é™…æ¨ç†æ—¶é—´åœ¨å¤–éƒ¨è®¡ç®—
        
        # è¿”å›åˆ—è¡¨å½¢å¼çš„æŒ‡æ ‡ï¼ˆä¸è®­ç»ƒè„šæœ¬æœŸæœ›çš„æ ¼å¼åŒ¹é…ï¼‰
        return {
            'ARI': ari_scores,
            'NMI': nmi_scores,
            'Modularity': modularity_scores,
            'Inference_Time_ms': inference_times
        }


def test_evaluation_metrics():
    """æµ‹è¯•è¯„ä¼°æŒ‡æ ‡è®¡ç®—"""
    print("ğŸ§ª æµ‹è¯•è¯„ä¼°æŒ‡æ ‡è®¡ç®—...")
    
    evaluator = GraphEvaluationMetrics()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    max_nodes = 50
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # æ¨¡æ‹Ÿæ‰¹æ¬¡æ•°æ®
    batch = {
        'image': torch.randn(batch_size, 1, 64, 64).to(device),
        'points': torch.randn(batch_size, max_nodes, 2).to(device),
        'adjacency': torch.rand(batch_size, max_nodes, max_nodes).to(device),
        'node_mask': torch.ones(batch_size, max_nodes, dtype=torch.bool).to(device)
    }
    
    # è®¾ç½®æœ‰æ•ˆèŠ‚ç‚¹æ•°
    batch['node_mask'][0, 30:] = False  # ç¬¬ä¸€ä¸ªæ ·æœ¬30ä¸ªèŠ‚ç‚¹
    batch['node_mask'][1, 25:] = False  # ç¬¬äºŒä¸ªæ ·æœ¬25ä¸ªèŠ‚ç‚¹
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•æ¨¡å‹
    class SimpleModel(torch.nn.Module):
        def forward(self, images, node_masks):
            batch_size, max_nodes = node_masks.shape
            return {
                'adjacency_matrix': torch.rand(batch_size, max_nodes, max_nodes).to(images.device)
            }
    
    model = SimpleModel().to(device)
    
    # è¯„ä¼°æŒ‡æ ‡
    metrics = evaluator.evaluate_batch(model, batch)
    
    print("âœ… è¯„ä¼°æŒ‡æ ‡ç»“æœ:")
    for key, value in metrics.items():
        print(f"  {key}: {value:.6f}")
    
    print("ğŸ‰ è¯„ä¼°æŒ‡æ ‡æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_evaluation_metrics()