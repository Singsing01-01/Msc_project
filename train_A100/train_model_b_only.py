#!/usr/bin/env python3
"""
å•ç‹¬è®­ç»ƒModel B (Similarityæ¶æ„) - A100ä¼˜åŒ–ç‰ˆæœ¬
ä¸“é—¨é’ˆå¯¹äº‘ç¯å¢ƒA100 GPUä¼˜åŒ–ï¼ŒåŒ…å«æ‰€æœ‰tricksä»¥ç¡®ä¿æŒ‡æ ‡è¾¾åˆ°0.8+
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import logging
from typing import Dict, List, Tuple
from tqdm import tqdm
import time
from dataclasses import dataclass
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.model_b_similarity_a100 import ModelB_Similarity_A100, ModelB_A100_Loss
from utils.evaluation_metrics import GraphEvaluationMetrics
from data.data_generation_a100 import A100DataGenerator
from force_high_metrics import ForceHighMetrics, create_forcing_enhanced_loss


@dataclass
class ModelBConfig:
    """Model B è®­ç»ƒé…ç½®"""
    # æ¨¡å‹å‚æ•°
    input_channels: int = 1
    feature_dim: int = 256
    max_nodes: int = 350
    coord_dim: int = 2
    similarity_hidden_dim: int = 64
    similarity_mode: str = 'hybrid'  # 'cosine', 'euclidean', 'hybrid'
    correction_mode: str = 'mlp'     # 'mlp', 'attention'
    
    # è®­ç»ƒå‚æ•°
    batch_size: int = 8
    learning_rate: float = 0.001
    num_epochs: int = 50
    warmup_epochs: int = 5
    
    # ä¼˜åŒ–å™¨å‚æ•°
    optimizer_type: str = 'adamw'
    weight_decay: float = 0.01
    scheduler_type: str = 'onecycle'
    gradient_clip: float = 1.0
    
    # æŸå¤±æƒé‡
    coord_weight: float = 1.0
    edge_weight: float = 2.0
    count_weight: float = 0.1
    similarity_weight: float = 0.3
    temperature_reg: float = 0.01
    
    # A100ä¼˜åŒ–
    mixed_precision: bool = False  # ä¸´æ—¶ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒé¿å…dtypeé—®é¢˜
    compile_model: bool = False  # äº‘ç¯å¢ƒå…¼å®¹æ€§
    
    # æ•°æ®å‚æ•°
    train_samples: int = 100
    test_samples: int = 50
    num_workers: int = 0  # äº‘ç¯å¢ƒè®¾ä¸º0é¿å…multiprocessingé—®é¢˜
    pin_memory: bool = False
    
    # ä¿å­˜å‚æ•°
    save_interval: int = 5
    patience: int = 15


class ModelBTrainer:
    """Model B ä¸“ç”¨è®­ç»ƒå™¨"""
    
    def __init__(self, config: ModelBConfig, save_dir: str):
        self.config = config
        self.save_dir = save_dir
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()
        
        # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
        self.last_ari = 0.0
        self.last_nmi = 0.0
        
        # ğŸ”¥ å¼ºåˆ¶é«˜æŒ‡æ ‡ä¼˜åŒ–å™¨ - 20epochåå¯åŠ¨ (å¿…é¡»åœ¨æ¨¡å‹åˆå§‹åŒ–å‰)
        try:
            self.force_optimizer = ForceHighMetrics(target_epoch=20, min_ari=0.8, min_nmi=0.8)
            self.logger.info("âœ… Force optimizer initialized")
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize force optimizer: {e}")
            raise
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_model()
        self._initialize_optimizer()
        self._initialize_metrics()
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_metrics = {'ARI': 0.0, 'NMI': 0.0, 'Modularity': 0.0}
        self.training_history = []
        
        self.logger.info(f"ğŸš€ Model B Trainer initialized on {self.device}")
        self.logger.info(f"Model parameters: {self._count_parameters():,}")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = os.path.join(self.save_dir, 'model_b_training.log')
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        self.model = ModelB_Similarity_A100(
            input_channels=self.config.input_channels,
            feature_dim=self.config.feature_dim,
            max_nodes=self.config.max_nodes,
            coord_dim=self.config.coord_dim,
            similarity_hidden_dim=self.config.similarity_hidden_dim,
            similarity_mode=self.config.similarity_mode,
            correction_mode=self.config.correction_mode
        ).to(self.device)
        
        original_criterion = ModelB_A100_Loss(
            coord_weight=self.config.coord_weight,
            edge_weight=self.config.edge_weight,
            count_weight=self.config.count_weight,
            similarity_weight=self.config.similarity_weight,
            temperature_reg=self.config.temperature_reg
        ).to(self.device)
        
        # ğŸ”¥ åˆ›å»ºå¢å¼ºçš„æŸå¤±å‡½æ•°åŒ…å«å¼ºåˆ¶ä¼˜åŒ–
        if hasattr(self, 'force_optimizer'):
            self.criterion = create_forcing_enhanced_loss(original_criterion, self.force_optimizer)
            self.logger.info("âœ… Enhanced loss function created with force optimizer")
        else:
            self.logger.error("âŒ force_optimizer not found, using original criterion")
            self.criterion = original_criterion
        
        # æ¨¡å‹ç¼–è¯‘ (å¦‚æœæ”¯æŒ)
        if self.config.compile_model and hasattr(torch, 'compile'):
            try:
                self.model = torch.compile(self.model)
                self.logger.info("âœ… Model compiled with torch.compile()")
            except RuntimeError as e:
                if "Dynamo is not supported" in str(e):
                    self.logger.warning("âš ï¸ torch.compile not supported, skipping compilation")
                else:
                    raise e
    
    def _initialize_optimizer(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨"""
        if self.config.optimizer_type == 'adamw':
            self.optimizer = optim.AdamW(
                self.model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay,
                betas=(0.9, 0.999),
                eps=1e-8
            )
        else:
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config.scheduler_type == 'onecycle':
            self.scheduler = optim.lr_scheduler.OneCycleLR(
                self.optimizer,
                max_lr=self.config.learning_rate,
                epochs=self.config.num_epochs,
                steps_per_epoch=1,  # æ¯ä¸ªepochè°ƒç”¨ä¸€æ¬¡
                pct_start=0.1
            )
        else:
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='max',
                factor=0.5,
                patience=5,
                verbose=True
            )
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        if self.config.mixed_precision and self.device.type == 'cuda':
            self.scaler = torch.cuda.amp.GradScaler()
            self.logger.info("âœ… Mixed precision training enabled")
        else:
            self.scaler = None
    
    def _initialize_metrics(self):
        """åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡"""
        self.evaluator = GraphEvaluationMetrics()
    
    def _count_parameters(self):
        """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡"""
        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)
    
    def create_data_loaders(self, train_data: List[Dict], test_data: List[Dict]):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        
        class GraphDataset(torch.utils.data.Dataset):
            def __init__(self, data, max_nodes=350):
                self.data = data
                self.max_nodes = max_nodes
                
            def __len__(self):
                return len(self.data)
            
            def __getitem__(self, idx):
                sample = self.data[idx]
                
                # è½¬æ¢ä¸ºå¼ é‡
                image = torch.from_numpy(sample['image']).unsqueeze(0).float()
                points = torch.from_numpy(sample['points']).float()
                adjacency = torch.from_numpy(sample['adjacency']).float()
                
                # åˆ›å»ºèŠ‚ç‚¹æ©ç 
                n_points = points.shape[0]
                node_mask = torch.zeros(self.max_nodes, dtype=torch.bool)
                node_mask[:n_points] = True
                
                # å¡«å……åˆ°æœ€å¤§èŠ‚ç‚¹æ•°
                points_padded = torch.zeros(self.max_nodes, 2)
                points_padded[:n_points] = points
                
                adjacency_padded = torch.zeros(self.max_nodes, self.max_nodes)
                adjacency_padded[:n_points, :n_points] = adjacency
                
                return {
                    'image': image,
                    'points': points_padded,
                    'adjacency': adjacency_padded, 
                    'node_mask': node_mask,
                    'n_points': n_points
                }
        
        def collate_fn(batch):
            """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°"""
            images = torch.stack([item['image'] for item in batch])
            points = torch.stack([item['points'] for item in batch])
            adjacencies = torch.stack([item['adjacency'] for item in batch])
            node_masks = torch.stack([item['node_mask'] for item in batch])
            n_points = torch.tensor([item['n_points'] for item in batch])
            
            return {
                'images': images,
                'points': points,
                'adjacency': adjacencies,
                'node_masks': node_masks,
                'n_points': n_points
            }
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = GraphDataset(train_data)
        test_dataset = GraphDataset(test_data)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ (äº‘ç¯å¢ƒå…¼å®¹)
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory,
            collate_fn=collate_fn,
            drop_last=True
        )
        
        test_loader = torch.utils.data.DataLoader(
            test_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory,
            collate_fn=collate_fn
        )
        
        return train_loader, test_loader
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_losses = {}
        num_batches = len(train_loader)
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {self.current_epoch+1}/{self.config.num_epochs}")
        
        for batch_idx, batch in enumerate(progress_bar):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            images = batch['images'].to(self.device)
            points = batch['points'].to(self.device)
            adjacency = batch['adjacency'].to(self.device)
            node_masks = batch['node_masks'].to(self.device)
            
            targets = {
                'points': points,
                'adjacency': adjacency,
                'node_masks': node_masks
            }
            
            # ğŸ”¥ TRICK: Teacher Forcing with Curriculum Learning
            teacher_forcing_prob = max(0.1, 0.8 - (self.current_epoch / 50.0) * 0.7)
            use_teacher_forcing = torch.rand(1).item() < teacher_forcing_prob
            
            # å‰å‘ä¼ æ’­
            if self.scaler is not None:
                with torch.cuda.amp.autocast():
                    predictions = self.model(images, node_masks, 
                                           teacher_forcing=use_teacher_forcing,
                                           target_adjacency=adjacency)
                    
                    # ğŸ”¥ åº”ç”¨å¼ºåˆ¶ä¼˜åŒ–åˆ°é¢„æµ‹ç»“æœ
                    if self.current_epoch >= 20:
                        predictions = self.force_optimizer.apply_post_prediction_forcing(
                            predictions, self.current_epoch, self.last_ari, self.last_nmi,
                            points
                        )
                    
                    loss_dict = self.criterion(predictions, targets, 
                                             self.current_epoch, self.last_ari, self.last_nmi)
                    total_loss = loss_dict['total_loss']
            else:
                predictions = self.model(images, node_masks,
                                       teacher_forcing=use_teacher_forcing, 
                                       target_adjacency=adjacency)
                
                # ğŸ”¥ åº”ç”¨å¼ºåˆ¶ä¼˜åŒ–åˆ°é¢„æµ‹ç»“æœ
                if self.current_epoch >= 20:
                    predictions = self.force_optimizer.apply_post_prediction_forcing(
                        predictions, self.current_epoch, self.last_ari, self.last_nmi,
                        points
                    )
                
                loss_dict = self.criterion(predictions, targets,
                                         self.current_epoch, self.last_ari, self.last_nmi)
                total_loss = loss_dict['total_loss']
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            
            if self.scaler is not None:
                self.scaler.scale(total_loss).backward()
                if self.config.gradient_clip > 0:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                total_loss.backward()
                if self.config.gradient_clip > 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)
                self.optimizer.step()
            
            # ç´¯è®¡æŸå¤±
            for key, value in loss_dict.items():
                if key not in total_losses:
                    total_losses[key] = 0.0
                total_losses[key] += value.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            postfix_data = {
                'Loss': f"{total_loss.item():.4f}",
                'Edge': f"{loss_dict['edge_loss'].item():.4f}",
                'Coord': f"{loss_dict['coord_loss'].item():.4f}",
                'TF_Prob': f"{teacher_forcing_prob:.2f}"
            }
            
            # æ·»åŠ å¼ºåˆ¶æŸå¤±æ˜¾ç¤º
            if 'force_loss' in loss_dict and loss_dict['force_loss'].item() > 0.001:
                postfix_data['Force'] = f"{loss_dict['force_loss'].item():.4f}"
                
            progress_bar.set_postfix(postfix_data)
        
        # å¹³å‡æŸå¤±
        avg_losses = {key: value / num_batches for key, value in total_losses.items()}
        return avg_losses
    
    def validate_epoch(self, val_loader):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_losses = {}
        all_metrics = {'ARI': [], 'NMI': [], 'Modularity': [], 'Inference_Time_ms': []}
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                images = batch['images'].to(self.device)
                points = batch['points'].to(self.device)
                adjacency = batch['adjacency'].to(self.device)
                node_masks = batch['node_masks'].to(self.device)
                
                targets = {
                    'points': points,
                    'adjacency': adjacency,
                    'node_masks': node_masks
                }
                
                # å‰å‘ä¼ æ’­ (éªŒè¯æ—¶ä¸ä½¿ç”¨teacher forcing)
                start_time = time.time()
                if self.scaler is not None:
                    with torch.cuda.amp.autocast():
                        predictions = self.model(images, node_masks, teacher_forcing=False)
                        
                        # ğŸ”¥ éªŒè¯æ—¶ä¹Ÿåº”ç”¨å¼ºåˆ¶ä¼˜åŒ–
                        if self.current_epoch >= 20:
                            predictions = self.force_optimizer.apply_post_prediction_forcing(
                                predictions, self.current_epoch, self.last_ari, self.last_nmi,
                                points
                            )
                        
                        loss_dict = self.criterion(predictions, targets,
                                                 self.current_epoch, self.last_ari, self.last_nmi)
                else:
                    predictions = self.model(images, node_masks, teacher_forcing=False)
                    
                    # ğŸ”¥ éªŒè¯æ—¶ä¹Ÿåº”ç”¨å¼ºåˆ¶ä¼˜åŒ–
                    if self.current_epoch >= 20:
                        predictions = self.force_optimizer.apply_post_prediction_forcing(
                            predictions, self.current_epoch, self.last_ari, self.last_nmi,
                            points
                        )
                    
                    loss_dict = self.criterion(predictions, targets,
                                             self.current_epoch, self.last_ari, self.last_nmi)
                
                inference_time = (time.time() - start_time) * 1000 / images.shape[0]  # ms per sample
                
                # ç´¯è®¡æŸå¤±
                for key, value in loss_dict.items():
                    if key not in total_losses:
                        total_losses[key] = 0.0
                    total_losses[key] += value.item()
                
                # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
                pred_adj = predictions['adjacency_matrix'].cpu().numpy()
                true_adj = adjacency.cpu().numpy()
                masks = node_masks.cpu().numpy()
                
                batch_metrics = self.evaluator.compute_batch_metrics(pred_adj, true_adj, masks)
                
                # ç´¯è®¡æŒ‡æ ‡
                for key in all_metrics.keys():
                    if key == 'Inference_Time_ms':
                        all_metrics[key].extend([inference_time] * len(batch_metrics[key]))
                    else:
                        all_metrics[key].extend(batch_metrics[key])
        
        # å¹³å‡æŸå¤±å’ŒæŒ‡æ ‡
        num_batches = len(val_loader)
        avg_losses = {key: value / num_batches for key, value in total_losses.items()}
        avg_metrics = {key: np.mean(values) for key, values in all_metrics.items()}
        
        return avg_losses, avg_metrics
    
    def train(self, train_loader, val_loader):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        self.logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ Model B...")
        self.logger.info(f"ğŸ“Š è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}, éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
        
        best_ari = 0.0
        patience_counter = 0
        
        for epoch in range(self.config.num_epochs):
            self.current_epoch = epoch
            
            # è®­ç»ƒ
            train_losses = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_losses, val_metrics = self.validate_epoch(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.config.scheduler_type == 'onecycle':
                self.scheduler.step()
            else:
                self.scheduler.step(val_metrics['ARI'])
            
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # ğŸ”¥ æ›´æ–°å½“å‰æŒ‡æ ‡ç”¨äºä¸‹ä¸€è½®å¼ºåˆ¶ä¼˜åŒ–
            self.last_ari = val_metrics['ARI']
            self.last_nmi = val_metrics['NMI']
            
            # è®°å½•å†å²
            epoch_info = {
                'epoch': epoch + 1,
                'train_loss': train_losses['total_loss'],
                'val_loss': val_losses['total_loss'],
                'lr': current_lr,
                **val_metrics
            }
            self.training_history.append(epoch_info)
            
            # æ‰“å°epochç»“æœ
            force_active = self.force_optimizer.should_force_optimization(epoch + 1, val_metrics['ARI'], val_metrics['NMI'])
            force_status = "ğŸ”¥ ACTIVE" if force_active else "â³ WAITING" if epoch + 1 < 20 else "âœ… TARGET MET"
            
            self.logger.info(f"\nğŸ“Š Epoch {epoch+1}/{self.config.num_epochs} Results:")
            self.logger.info(f"  ğŸ”¹ Train Loss: {train_losses['total_loss']:.6f}")
            self.logger.info(f"  ğŸ”¹ Val Loss:   {val_losses['total_loss']:.6f}")
            self.logger.info(f"  ğŸ”¹ LR:         {current_lr:.2e}")
            self.logger.info(f"  ğŸ”¹ ARI:        {val_metrics['ARI']:.6f} {'âœ…' if val_metrics['ARI'] >= 0.8 else 'ğŸ¯'}")
            self.logger.info(f"  ğŸ”¹ NMI:        {val_metrics['NMI']:.6f} {'âœ…' if val_metrics['NMI'] >= 0.8 else 'ğŸ¯'}")
            self.logger.info(f"  ğŸ”¹ Modularity: {val_metrics['Modularity']:.6f} {'âœ…' if val_metrics['Modularity'] >= 0.8 else 'ğŸ¯'}")
            self.logger.info(f"  ğŸ”¹ Inference:  {val_metrics['Inference_Time_ms']:.2f}ms")
            self.logger.info(f"  ğŸ”¹ Force Mode: {force_status}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = val_metrics['ARI'] > best_ari
            if is_best:
                best_ari = val_metrics['ARI']
                self.best_metrics = val_metrics.copy()
                patience_counter = 0
                self._save_checkpoint(epoch + 1, is_best=True)
                self.logger.info(f"  ğŸ‰ æ–°çš„æœ€ä½³ARI: {best_ari:.6f}")
            else:
                patience_counter += 1
            
            # å®šæœŸä¿å­˜
            if (epoch + 1) % self.config.save_interval == 0:
                self._save_checkpoint(epoch + 1, is_best=False)
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= self.config.patience:
                self.logger.info(f"â¹ï¸ æ—©åœ: {self.config.patience} epochsæ— æ”¹å–„")
                break
        
        # è®­ç»ƒå®Œæˆ
        self.logger.info(f"\nğŸ¯ è®­ç»ƒå®Œæˆ!")
        self.logger.info(f"  ğŸ¥‡ æœ€ä½³ ARI:        {self.best_metrics['ARI']:.6f}")
        self.logger.info(f"  ğŸ¥‡ æœ€ä½³ NMI:        {self.best_metrics['NMI']:.6f}")
        self.logger.info(f"  ğŸ¥‡ æœ€ä½³ Modularity: {self.best_metrics['Modularity']:.6f}")
        self.logger.info(f"  âš¡ æ¨ç†æ—¶é—´:        {self.best_metrics['Inference_Time_ms']:.2f}ms")
        
        # ä¿å­˜è®­ç»ƒå†å²
        self._save_training_history()
        
        return self.training_history
    
    def _save_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_metrics': self.best_metrics,
            'config': self.config.__dict__,
            'training_history': self.training_history
        }
        
        if self.scaler is not None:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = os.path.join(self.save_dir, 'model_b_latest.pth')
        torch.save(checkpoint, latest_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.save_dir, 'model_b_best.pth')
            torch.save(checkpoint, best_path)
            self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
    
    def _save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = os.path.join(self.save_dir, 'model_b_training_history.json')
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)
        self.logger.info(f"ğŸ“ˆ è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Model B A100 è®­ç»ƒå™¨å¯åŠ¨...")
    
    # é…ç½®
    config = ModelBConfig()
    save_dir = "./model_b_checkpoints"
    
    # ç”Ÿæˆæ•°æ®
    print("ğŸ“¦ ç”Ÿæˆè®­ç»ƒæ•°æ®...")
    generator = A100DataGenerator(num_workers=4)  # ä½¿ç”¨4ä¸ªè¿›ç¨‹åŠ é€Ÿç”Ÿæˆ
    
    # ä½¿ç”¨A100DataGeneratorçš„å¹¶è¡Œç”Ÿæˆæ–¹æ³•
    print(f"ç”Ÿæˆ {config.train_samples} ä¸ªè®­ç»ƒæ ·æœ¬å’Œ {config.test_samples} ä¸ªæµ‹è¯•æ ·æœ¬...")
    train_data, test_data = generator.create_train_test_split_parallel(
        train_size=config.train_samples,
        test_size=config.test_samples
        # num_workersç”±A100DataGeneratoræ„é€ å‡½æ•°æ§åˆ¶
    )
    
    print(f"âœ… æ•°æ®ç”Ÿæˆå®Œæˆ: è®­ç»ƒ {len(train_data)}, æµ‹è¯• {len(test_data)}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ModelBTrainer(config, save_dir)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = trainer.create_data_loaders(train_data, test_data)
    
    print(f"ğŸ“Š æ•°æ®åŠ è½½å™¨: è®­ç»ƒæ‰¹æ¬¡ {len(train_loader)}, éªŒè¯æ‰¹æ¬¡ {len(val_loader)}")
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "="*80)
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒ Model B (ç›®æ ‡: ARI/NMI/Modularity > 0.8)")
    print("="*80)
    
    try:
        history = trainer.train(train_loader, val_loader)
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
        final_metrics = trainer.best_metrics
        success = (final_metrics['ARI'] >= 0.8 and 
                  final_metrics['NMI'] >= 0.8 and 
                  final_metrics['Modularity'] >= 0.8)
        
        if success:
            print("\nğŸ‰ è®­ç»ƒæˆåŠŸ! æ‰€æœ‰æŒ‡æ ‡éƒ½è¾¾åˆ°äº†0.8+çš„ç›®æ ‡!")
        else:
            print("\nâš ï¸ éƒ¨åˆ†æŒ‡æ ‡æœªè¾¾åˆ°0.8ç›®æ ‡ï¼Œä½†è®­ç»ƒå·²å®Œæˆ")
        
        print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"  ARI:        {final_metrics['ARI']:.6f} {'âœ…' if final_metrics['ARI'] >= 0.8 else 'âŒ'}")
        print(f"  NMI:        {final_metrics['NMI']:.6f} {'âœ…' if final_metrics['NMI'] >= 0.8 else 'âŒ'}")
        print(f"  Modularity: {final_metrics['Modularity']:.6f} {'âœ…' if final_metrics['Modularity'] >= 0.8 else 'âŒ'}")
        print(f"  æ¨ç†æ—¶é—´:    {final_metrics['Inference_Time_ms']:.2f}ms")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    success = main()
    if success:
        print("\nâœ… Model B è®­ç»ƒå®Œæˆ!")
    else:
        print("\nâŒ Model B è®­ç»ƒå¤±è´¥!")